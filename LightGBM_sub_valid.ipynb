{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas\n",
    "# !pip install --upgrade google-api-python-client\n",
    "# !pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.io import gbq\n",
    "from pandas_gbq import read_gbq\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "# from pyspark.sql import functions as sfunc\n",
    "# from pyspark.sql import types as stypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE = False\n",
    "\n",
    "MAX_ROUNDS = 2000\n",
    "EARLY_STOP = 50\n",
    "OPT_ROUNDS = 2000\n",
    "\n",
    "FULL_OUTFILE = 'lgbm_r_to_python_nocv_150upsample_sub.csv'\n",
    "#VALID_OUTFILE = 'sub_lgbm_r_to_python_withcv_150upsample.csv'\n",
    "VALID_OUTFILE = 'lgbm_r_to_python_withcv_150upsample_valid.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "\n",
    "path = ''\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "print('load train...')\n",
    "train_cols = ['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "train_df = pd.read_csv(path+\"train.csv\", dtype=dtypes, usecols=train_cols)\n",
    "\n",
    "#train_df = pd.read_csv(path+\"train.csv\", skiprows=range(1,84903891), nrows=100000000,dtype=dtypes, usecols=train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '../valid-train_data/'\n",
    "valid_df = pd.read_pickle(path1+'validation.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "len_train = len(train_df)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('data prep...')\n",
    "\n",
    "most_freq_hours_in_test_data = [4, 5, 9, 10, 13, 14]\n",
    "least_freq_hours_in_test_data = [6, 11, 15]\n",
    "\n",
    "\n",
    "def prep_data( df ):\n",
    "    \n",
    "    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('uint8')\n",
    "    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('uint8')\n",
    "    df.drop(['click_time'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    df['in_test_hh'] = (   3 \n",
    "                         - 2*df['hour'].isin(  most_freq_hours_in_test_data ) \n",
    "                         - 1*df['hour'].isin( least_freq_hours_in_test_data ) ).astype('uint8')\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_day_test_hh')\n",
    "    gp = df[['ip', 'day', 'in_test_hh', 'channel']].groupby(by=['ip', 'day',\n",
    "             'in_test_hh'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_day_test_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','in_test_hh'], how='left')\n",
    "    del gp\n",
    "    df.drop(['in_test_hh'], axis=1, inplace=True)\n",
    "    print( \"nip_day_test_hh max value = \", df.nip_day_test_hh.max() )\n",
    "    df['nip_day_test_hh'] = df['nip_day_test_hh'].astype('uint32')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_day_hh')\n",
    "    gp = df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', \n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_day_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_day_hh max value = \", df.nip_day_hh.max() )\n",
    "    df['nip_day_hh'] = df['nip_day_hh'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_os')\n",
    "    gp = df[['ip', 'day', 'os', 'hour', 'channel']].groupby(by=['ip', 'os', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_os'})\n",
    "    df = df.merge(gp, on=['ip','os','hour','day'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_os max value = \", df.nip_hh_os.max() )\n",
    "    df['nip_hh_os'] = df['nip_hh_os'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_app')\n",
    "    gp = df[['ip', 'app', 'hour', 'day', 'channel']].groupby(by=['ip', 'app', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_app'})\n",
    "    df = df.merge(gp, on=['ip','app','hour','day'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_app max value = \", df.nip_hh_app.max() )\n",
    "    df['nip_hh_app'] = df['nip_hh_app'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_dev')\n",
    "    gp = df[['ip', 'device', 'hour', 'day', 'channel']].groupby(by=['ip', 'device', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_dev'})\n",
    "    df = df.merge(gp, on=['ip','device','day','hour'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_dev max value = \", df.nip_hh_dev.max() )\n",
    "    df['nip_hh_dev'] = df['nip_hh_dev'].astype('uint32')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    df.drop( ['ip','day'], axis=1, inplace=True )\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "    \n",
    "    return( df )\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "print( \"Train info before: \")\n",
    "print( train_df.info() )\n",
    "train_df = prep_data( train_df )\n",
    "gc.collect()\n",
    "print( \"Train info after: \")\n",
    "print( train_df.info() )\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = 'auc'\n",
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 4,  # -1 means no limit\n",
    "        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'nthread': -1,\n",
    "        'verbose': 0,\n",
    "        'scale_pos_weight':150, # because training data is extremely unbalanced \n",
    "        'metric':metrics\n",
    "}\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh',\n",
    "              'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATE:\n",
    "    \n",
    "    train_df, val_df = train_test_split( train_df, train_size=.95, shuffle=False )\n",
    "\n",
    "    print(train_df.info())\n",
    "    print(val_df.info())\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    num_boost_round=MAX_ROUNDS\n",
    "    early_stopping_rounds=EARLY_STOP\n",
    "    \n",
    "    xgtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    xgvalid = lgb.Dataset(val_df[predictors].values, label=val_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets= [xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=None)\n",
    "\n",
    "    n_estimators = bst.best_iteration\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "    \n",
    "    outfile = VALID_OUTFILE\n",
    "    \n",
    "    del xgvalid\n",
    "\n",
    "else:\n",
    "\n",
    "    print(train_df.info())\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    num_boost_round=OPT_ROUNDS\n",
    "\n",
    "    xgtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    bst = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     verbose_eval=10, \n",
    "                     feval=None)\n",
    "\n",
    "    outfile = FULL_OUTFILE\n",
    "    valid_file = VALID_OUTFILE\n",
    "del xgtrain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('load test...')\n",
    "test_cols = ['ip','app','device','os', 'channel', 'click_time', 'click_id']\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=test_cols)\n",
    "\n",
    "test_df = prep_data( test_df )\n",
    "valid_df = prep_data( valid_df )\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id']\n",
    "\n",
    "print(\"Predicting...\")\n",
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "\n",
    "print(\"writing...\")\n",
    "sub.to_csv(path1+outfile, index=False, float_format='%.9f')\n",
    "\n",
    "print(\"done...\")\n",
    "print(sub.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['is_attributed'] = bst.predict(valid_df[predictors])\n",
    "val.to_csv(path1+valid_file, index=False, float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
