{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas\n",
    "# !pip install --upgrade google-api-python-client\n",
    "# !pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.io import gbq\n",
    "from pandas_gbq import read_gbq\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from pyspark.sql import functions as sfunc\n",
    "# from pyspark.sql import types as stypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BQ_Connect-FTRL.ipynb\r\n",
      "catboost_training.json\r\n",
      "Catboost_v1-More feat.ipynb\r\n",
      "DL_Support_v1.ipynb\r\n",
      "ftrl_submission.csv\r\n",
      "Kaggle-TalkingData-f0e6b0d0e656.json\r\n",
      "KNN_v1.ipynb\r\n",
      "learn\r\n",
      "learn_error.tsv\r\n",
      "LGBM_99.ipynb\r\n",
      "LGBM_v1.ipynb\r\n",
      "LGBM_v1-More feat.ipynb\r\n",
      "LGBM_v2.ipynb\r\n",
      "LightGBM.ipynb\r\n",
      "meta.tsv\r\n",
      "RF_Medium_5.ipynb\r\n",
      "sub_lgbm_r_to_python_nocv_150upsample.csv\r\n",
      "sub_lgbm_r_to_python_nocv_170upsample_maxdepth.csv\r\n",
      "sub_lgbm_r_to_python_nocv.csv\r\n",
      "sub_xgb_nocv_150upsample.csv\r\n",
      "test.csv\r\n",
      "time_left.tsv\r\n",
      "train.csv\r\n",
      "Untitled1.ipynb\r\n",
      "Untitled2.ipynb\r\n",
      "Untitled3.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "Valid-data_generation.ipynb\r\n",
      "Wordbatch.ipynb\r\n",
      "Xgboost.ipynb\r\n",
      "XGB_v1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train...\n"
     ]
    }
   ],
   "source": [
    "VALIDATE = False\n",
    "\n",
    "MAX_ROUNDS = 2000\n",
    "EARLY_STOP = 50\n",
    "OPT_ROUNDS = 2000\n",
    "\n",
    "FULL_OUTFILE = 'morefeat_rf_medium_sub.csv'\n",
    "#VALID_OUTFILE = 'sub_lgbm_r_to_python_withcv_150upsample.csv'\n",
    "VALID_OUTFILE = 'morefeat_rf_medium_valid.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "\n",
    "path = ''\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "print('load train...')\n",
    "train_cols = ['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "train_df = pd.read_csv(path+\"train.csv\", dtype=dtypes, skiprows=range(1,144903891), nrows=40000000,usecols=train_cols)\n",
    "\n",
    "#train_df = pd.read_csv(path+\"train.csv\", skiprows=range(1,84903891), nrows=100000000,dtype=dtypes, usecols=train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '../valid-train_data/'\n",
    "valid_df = pd.read_pickle(path1+'validation.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "len_train = len(train_df)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('data prep...')\n",
    "\n",
    "most_freq_hours_in_test_data    = [4, 5, 9, 10, 13, 14]\n",
    "middle1_freq_hours_in_test_data = [16, 17, 22]\n",
    "least_freq_hours_in_test_data   = [6, 11, 15]\n",
    "\n",
    "\n",
    "def shift_feat(df):\n",
    "    df = df.sort_values(['ip', 'click_time'])\n",
    "    df['click_time'] = pd.to_datetime(df.click_time)\n",
    "    df['ip_time_diff'] = df['click_time'].diff().dt.seconds\n",
    "    df.loc[df.ip != df.ip.shift(), 'ip_time_diff'] = -1\n",
    "    df = df.sort_values(['ip', 'app','device','os','channel', 'click_time'])\n",
    "    df['all_time_diff'] = df['click_time'].diff().dt.seconds\n",
    "    df.loc[df.channel != df.channel.shift(), 'all_time_diff'] = -1\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def prep_data( df ):\n",
    "    \n",
    "    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('uint8')\n",
    "    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('uint8')\n",
    "    df.drop(['click_time'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    df['in_test_hh'] = (   4 \n",
    "                         - 3*df['hour'].isin(  most_freq_hours_in_test_data ) \n",
    "                         - 2*df['hour'].isin(  middle1_freq_hours_in_test_data ) \n",
    "                         - 1*df['hour'].isin( least_freq_hours_in_test_data ) ).astype('uint8')\n",
    "    print( df.info() )\n",
    "    \n",
    " \n",
    "    print('group by : ip_day_test_hh')\n",
    "    gp = df[['ip', 'day', 'in_test_hh', 'channel']].groupby(by=['ip', 'day',\n",
    "             'in_test_hh'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_day_test_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','in_test_hh'], how='left')\n",
    "    del gp\n",
    "    df.drop(['in_test_hh'], axis=1, inplace=True)\n",
    "    print( \"nip_day_test_hh max value = \", df.nip_day_test_hh.max() )\n",
    "    df['nip_day_test_hh'] = df['nip_day_test_hh'].astype('uint32')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_day_hh')\n",
    "    gp = df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', \n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_day_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_day_hh max value = \", df.nip_day_hh.max() )\n",
    "    df['nip_day_hh'] = df['nip_day_hh'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_os')\n",
    "    gp = df[['ip', 'day', 'os', 'hour', 'channel']].groupby(by=['ip', 'os', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_os'})\n",
    "    df = df.merge(gp, on=['ip','os','hour','day'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_os max value = \", df.nip_hh_os.max() )\n",
    "    df['nip_hh_os'] = df['nip_hh_os'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_app')\n",
    "    gp = df[['ip', 'app', 'hour', 'day', 'channel']].groupby(by=['ip', 'app', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_app'})\n",
    "    df = df.merge(gp, on=['ip','app','hour','day'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_app max value = \", df.nip_hh_app.max() )\n",
    "    df['nip_hh_app'] = df['nip_hh_app'].astype('uint16')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    print('group by : ip_hh_dev')\n",
    "    gp = df[['ip', 'device', 'hour', 'day', 'channel']].groupby(by=['ip', 'device', 'day',\n",
    "             'hour'])[['channel']].count().reset_index().rename(index=str, \n",
    "             columns={'channel': 'nip_hh_dev'})\n",
    "    df = df.merge(gp, on=['ip','device','day','hour'], how='left')\n",
    "    del gp\n",
    "    print( \"nip_hh_dev max value = \", df.nip_hh_dev.max() )\n",
    "    df['nip_hh_dev'] = df['nip_hh_dev'].astype('uint32')\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "\n",
    "    df.drop( ['ip','day'], axis=1, inplace=True )\n",
    "    gc.collect()\n",
    "    print( df.info() )\n",
    "    \n",
    "    return( df )\n",
    "\n",
    "#---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Train info before: \")\n",
    "print( train_df.info() )\n",
    "#train_df = shift_feat(train_df)\n",
    "train_df = prep_data( train_df )\n",
    "gc.collect()\n",
    "print( \"Train info after: \")\n",
    "print( train_df.info() )\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = 'auc'\n",
    "#clf = RandomForestClassifier(n_estimators=25)\n",
    "\n",
    "rf_params = {\n",
    "        'n_estimators': '1000',\n",
    "        'criterion': 'gini',\n",
    "        #'metric':metrics,\n",
    "        #'learning_rate': 0.1,\n",
    "        'max_depth': 4,  # -1 means no limit\n",
    "        #'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        #'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_samples_split':100,\n",
    "        'nthread': -1,\n",
    "        'verbose': 0,\n",
    "        'n_jobs':-1,\n",
    "        'random_state':1234,\n",
    "        'verbose':1,\n",
    "        'class_weight':{0:1,1:150}\n",
    "        #'scale_pos_weight':150 # because training data is extremely unbalanced \n",
    "}\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh',\n",
    "              'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
    "# predictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh',\n",
    "#               'nip_hh_os', 'nip_hh_app', 'nip_hh_dev','ip_time_diff','all_time_diff']\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATE:\n",
    "    \n",
    "    train_df, val_df = train_test_split( train_df, train_size=.95, shuffle=False)\n",
    "\n",
    "    print(train_df.info())\n",
    "    print(val_df.info())\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    num_boost_round=MAX_ROUNDS\n",
    "    early_stopping_rounds=EARLY_STOP\n",
    "    \n",
    "    xgtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    xgvalid = lgb.Dataset(val_df[predictors].values, label=val_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets= [xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=None)\n",
    "\n",
    "    n_estimators = bst.best_iteration\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "    \n",
    "    outfile = VALID_OUTFILE\n",
    "    \n",
    "    del xgvalid\n",
    "\n",
    "else:\n",
    "\n",
    "    print(train_df.info())\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    num_boost_round=OPT_ROUNDS\n",
    "    \n",
    "#     xgtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "#                           feature_name=predictors,\n",
    "#                           categorical_feature=categorical\n",
    "#                           )\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=5, n_jobs=-1, \n",
    "                           random_state=1234, verbose=1, warm_start=False, class_weight={0:1,1:150})\n",
    "    bst = clf.fit(train_df[predictors].values,train_df[target].values)\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "#     bst = lgb.train(lgb_params, \n",
    "#                      xgtrain, \n",
    "#                      num_boost_round=num_boost_round,\n",
    "#                      verbose_eval=10, \n",
    "#                      feval=None)\n",
    "\n",
    "    outfile = FULL_OUTFILE\n",
    "    valid_file = VALID_OUTFILE\n",
    "#del xgtrain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('load test...')\n",
    "test_cols = ['ip','app','device','os', 'channel', 'click_time', 'click_id']\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=test_cols)\n",
    "\n",
    "test_df = prep_data( test_df )\n",
    "valid_df = prep_data( valid_df )\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id']\n",
    "\n",
    "print(\"Predicting...\")\n",
    "a = bst.predict_proba(test_df[predictors].values)\n",
    "a = pd.DataFrame(a)\n",
    "a.columns = ['no','yes']\n",
    "sub['is_attributed'] = a['yes'].values\n",
    "print(\"writing...\")\n",
    "sub.to_csv(path1+outfile, index=False, float_format='%.9f')\n",
    "\n",
    "print(\"done...\")\n",
    "print(sub.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bst.predict_proba(test_df[predictors].values)\n",
    "a = pd.DataFrame(a)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "b = bst.predict_proba(valid_df[predictors].values)\n",
    "b = pd.DataFrame(b)\n",
    "b.columns = ['no','yes']\n",
    "val['is_attributed'] = b['yes'].values    \n",
    "val.to_csv(path1+valid_file, index=False, float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
